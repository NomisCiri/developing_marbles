res <- matrix(NA, n+1, length(x))
res[1,] <- x
for (i in seq_len(n))
res[i+1,] <- x <- x %*% P
res
}
n <- 10
y1 <- iterate.P(c(1, 0, 0), P, n)# x is a vector of possible starting states.
#Similarly, for the other two possible starting states:
y2 <- iterate.P(c(0, 1, 0), P, n)
y3 <- iterate.P(c(0, 0, 1), P, n)
#This shows the convergence on the stationary distribution.
matplot(0:n, y1, type="l", lty=1, xlab="Step", ylab="y", las=1)
matlines(0:n, y2, lty=2)
matlines(0:n, y3, lty=3)
#which means that regardless of the starting distribution, there is a 32\% chance of the chain being in state 1 after about 10 or more iterations regardless
#of where it started. So, knowing about the state of this ch
#The proceedure above iterated the overall probabilities of different states; not the actual transitions through the system. So, let’s iterate the system, rather than the probability vector.
#The function run here takes a state (this time, just an integer indicating which of the states $1, 2, 3$ the system is in),
#the same transition matrix as above, and a number of steps to run. Each step, it looks at the possible places that it could transition to and chooses 1 (this uses R’s sample function).
run <- function(i, P, n) {
res <- integer(n)
for (t in seq_len(n))
res[[t]] <- i <- sample(nrow(P), 1, pr=P[i,])
res
}
#Here’s the chain running around for 100 steps:
samples <- run(1, P, 100)
plot(samples, type="s", xlab="Step", ylab="State", las=1)
is a contrived example, but distributions like this are not totally impossible, and might arise when sampling things from a mixture (such as human heights, which are bimodal due to sexual dimorphism).
#Fairly arbitrarily, here are some parameters and the definition of the target density.
#
p <- 0.4
mu <- c(-1, 2)
sd <- c(.5, 2)
f <- function(x)
p     * dnorm(x, mu[1], sd[1]) +
(1-p) * dnorm(x, mu[2], sd[2])
#Here is the probability density plotted over the “important” part of the domain (in general, this may not even be known!)
curve(f(x), col="red", -4, 8, n=301, las=1)
q <- function(x) rnorm(1, x, 4)
#This implements the core algorithm, as described above:
step <- function(x, f, q) {
## Pick new point
xp <- q(x)
## Acceptance probability:
alpha <- min(1, f(xp) / f(x))
## Accept new point with probability alpha:
if (runif(1) < alpha)
x <- xp
## Returning the point:
x
}
#And this just takes care of running the MCMC for a number of steps. It will start at point x return a matrix with nsteps rows and the same number of columns as x has elements. If run on scalar x it will return a vector.
run <- function(x, f, q, nsteps) {
res <- matrix(NA, nsteps, length(x))
for (i in seq_len(nsteps))
res[i,] <- x <- step(x, f, q)
drop(res)
}
#We pick a place to start (how about -10, just to pick a really poor point)
res <- run(-10, f, q, 1000)
#Here are the first 1000 steps of the Markov chain, with the target density on the right:
layout(matrix(c(1, 2), 1, 2), widths=c(4, 1))
par(mar=c(4.1, .5, .5, .5), oma=c(0, 4.1, 0, 0))
plot(res, type="s", xpd=NA, ylab="Parameter", xlab="Sample", las=1)
usr <- par("usr")
xx <- seq(usr[3], usr[4], length=301)
plot(f(xx), xx, type="l", yaxs="i", axes=FALSE, xlab="")
usr
par()
#Even with only a thousand (non-independent) samples, we’re starting to resemble the target distribution fairly well.
plot.new()
hist(res, 50, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
#Run for longer and things start looking a bunch better:
set.seed(1)
res.long <- run(-10, f, q, 50000)#
plot.new()
hist(res.long, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density", col="grey")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
#Even with only a thousand (non-independent) samples, we’re starting to resemble the target distribution fairly well.
plot.new()
hist(res, 50, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
#Run for longer and things start looking a bunch better:
set.seed(1)
res.long <- run(-10, f, q, 50000)#
plot.new()
hist(res.long, 100, freq=FALSE, main="", ylim=c(0, .4), las=1,
xlab="x", ylab="Probability density", col="grey")
z <- integrate(f, -Inf, Inf)$value
curve(f(x) / z, add=TRUE, col="red", n=200)
install.packages("future")
install.packages("listenv")
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
login
plan(login) # this tells the future package that we want to use a remote server for running our future operations
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
plan(login) # this tells the future pa
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @plan(login) # this tells the future package that we want to use a remote server for running our future operations
plan(login) # this tells the future package that we want to use a remote server for running our future operations
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @plan(login) # this tells the future package that we want to use a remote server for running our future operations
plan(login) # this tells the future package that we want to use a remote server for running our future operations
# but maybe the reason we want to shift to a server is not that we want to run a simple sequential operation there.
# maybe we want to take advantage of parallelisation and run the operation in parallel on multiple cores
## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
login,
multicore,
sequential
))
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @plan(login) # this tells the future package that we want to use a remote server for running our future operations
plan(login) # this tells the future package that we want to use a remote server for running our future operations
knitr::opts_chunk$set(echo = TRUE)
# vignette("future-1-overview")
library(future)
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
plan(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # this tells the future package that we want to use a remote server for running our future operations
plan(remote, workers = "ciranka@arc-srv-cpt7") # this tells the future package that we want to use a remote server for running our future operations
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7.mpib-berlin.mpg.de") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
library("future") # this is the package we use to flexibly scale our processing
plan(login) # this tells the future package that we want to use a remote server for running our future operations
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
plan(login) # this tells the future package that we want to use a remote server for running our future operations
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt3") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt3") # Your username goes before the @
plan(login) # this tells the future package that
# SC: load the data from wouters experiment.
setwd("/Users/ciranka/Documents/R/SocialModels/Uncertainty/MR_behav/")
allfiles<-list.files()#get all files in the working directory.
relevantFiles<-list()#create an object where you store the relevant files.
#first identify the relevant files with the relevant ending
ending="_MRI.txt" # thats the file end you need to look for.
j=1# set a counter which counts hits seperately.
for(i in 1:length(allfiles)){
relevant<-substr(allfiles[i],nchar(allfiles[i])-7,nchar(allfiles[i]))
if (relevant == ending){
relevantFiles[j]=allfiles[i]
j=j+1
}
}
marbleData<-data.frame()
#here there was a tiny problem. Raw data txt files had varying header lengths. i deleted them manually but for recap:
#odd ones out are:85,70,47,38,31,19,4
for(i in 1:length(relevantFiles)){
#because everyone has this habit of not saving the subjectspecifics in the data, retrieve them from the text file.
subi<-read.table(as.character(relevantFiles[i]),skip=1,header=T)#
subi$subject<-substr(as.character(relevantFiles[i]),4,7)#between character 4 and 7 the subjnumber is hidden
subi$run<-substr(as.character(relevantFiles[i]),9,9)#at char 9 here is the run hidden
marbleData<-rbind(marbleData,subi)#concatenate.
}
#this is a function to model bayesian learning about probabilites of a binomal distribution.
#agents probability representation are assumed to be Beta distributed with mean as probability its variance as
#uncertainty. for the time beeing i need to draw random samples from the true probabilites and use them as if they would be the real values seen by the participants.
#prior is Jeffreys.
#there is no magic: The distributional notation is even simpler. Take the prior, Beta(α, β), and add the successes from the data, x, to α and the failures, n – x, to β, and there’s your posterior, Beta(α+x, β+n-x).
nSamples=10
p=0.4
lr=1
binomalLearning<-function(nSamples,p,lr){
obs<-rbinom(nSamples,1,p)
prior<-matrix(0.5,1,2)#shape parameters of the beta distribution.
for(i in 1:length(obs)){
if (as.logical(obs[i])==T){
prior=rbind(prior,c(lr*1+prior[i,1],prior[i,2]))
}#if sub saw positive outcome add a lr weighted value to the
else if (as.logical(obs[i])==F){
prior=rbind(prior,c(prior[i,1],lr*1+prior[i,2]))
}#if sub saw positive outcome add a lr weighted value to the
}
cbind(prior,append(obs,99,after=0))#beta shape parameter and observation.
}
sub<-binomalLearning(nSamples,p,lr)
pl.beta(sub[nrow(sub),1],sub[nrow(sub),2])
## Visualization, including limit cases:
pl.beta <- function(a,b, asp = if(isLim) 1, ylim = if(isLim) c(0,1.1)) {
if(isLim <- a == 0 || b == 0 || a == Inf || b == Inf) {
eps <- 1e-10
x <- c(0, eps, (1:7)/16, 1/2+c(-eps,0,eps), (9:15)/16, 1-eps, 1)
} else {
x <- seq(0, 1, length = 1025)
}
fx <- cbind(dbeta(x, a,b), pbeta(x, a,b), qbeta(x, a,b))
f <- fx; f[fx == Inf] <- 1e100
matplot(x, f, ylab="", type="l", ylim=ylim, asp=asp,
main = sprintf("[dpq]beta(x, a=%g, b=%g)", a,b))
abline(0,1,     col="gray", lty=3)
abline(h = 0:1, col="gray", lty=3)
legend("top", paste0(c("d","p","q"), "beta(x, a,b)"),
col=1:3, lty=1:3, bty = "n")
invisible(cbind(x, fx))
}
library("future") # this is the package we use to flexibly scale our processing
library("listenv") # this is a helper package that allows us to assign the results to a list even if they come in at different times
## Set up access to remote login node
login <- tweak(remote, workers = "ciranka@arc-srv-cpt7") # Your username goes before the @
plan(login) # this tells the future package that we want to use a remote server for running our future operations
# but maybe the reason we want to shift to a server is not that we want to run a simple sequential operation there.
# maybe we want to take advantage of parallelisation and run the operation in parallel on multiple cores
## Specify future topology
## login node -> { cluster nodes } -> { multiple cores }
plan(list(
login,
multicore,
sequential
))
# demo("mandelbrot", package = "future", ask = FALSE)
## (a) This will be evaluated on the cluster login computer
x %<-% { # this operator binds the result of the promise/future to x
thost <- Sys.info()[["nodename"]]
tpid <- Sys.getpid()
y <- listenv()
for (task in 1:4) {
## (b) This will be evaluated on a different core
y[[task]] %<-% {
mhost <- Sys.info()[["nodename"]]
mpid <- Sys.getpid()
z <- listenv()
for (jj in 1:2) {
## (c) this will be evaluated sequentially on the same core
z[[jj]] %<-% data.frame(task = task,
top.host = thost, top.pid = tpid,
mid.host = mhost, mid.pid = mpid,
host = Sys.info()[["nodename"]],
pid = Sys.getpid())
}
Reduce(rbind, z)
}
}
Reduce(rbind, y)
}
print(x)
# SC: load the data from wouters experiment.
setwd("/Users/ciranka/Documents/R/SocialModels/Uncertainty/MR_behav/")
allfiles<-list.files()#get all files in the working directory.
relevantFiles<-list()#create an object where you store the relevant files.
#first identify the relevant files with the relevant ending
ending="_MRI.txt" # thats the file end you need to look for.
j=1# set a counter which counts hits seperately.
for(i in 1:length(allfiles)){
relevant<-substr(allfiles[i],nchar(allfiles[i])-7,nchar(allfiles[i]))
if (relevant == ending){
relevantFiles[j]=allfiles[i]
j=j+1
}
}
marbleData<-data.frame()
#here there was a tiny problem. Raw data txt files had varying header lengths. i deleted them manually but for recap:
#odd ones out are:85,70,47,38,31,19,4
for(i in 1:length(relevantFiles)){
#because everyone has this habit of not saving the subjectspecifics in the data, retrieve them from the text file.
subi<-read.table(as.character(relevantFiles[i]),skip=1,header=T)#
subi$subject<-substr(as.character(relevantFiles[i]),4,7)#between character 4 and 7 the subjnumber is hidden
subi$run<-substr(as.character(relevantFiles[i]),9,9)#at char 9 here is the run hidden
marbleData<-rbind(marbleData,subi)#concatenate.
}
#this is a function to model bayesian learning about probabilites of a binomal distribution.
#agents probability representation are assumed to be Beta distributed with mean as probability its variance as
#uncertainty. for the time beeing i need to draw random samples from the true probabilites and use them as if they would be the real values seen by the participants.
#prior is Jeffreys.
#there is no magic: The distributional notation is even simpler. Take the prior, Beta(α, β), and add the successes from the data, x, to α and the failures, n – x, to β, and there’s your posterior, Beta(α+x, β+n-x).
nSamples=10
p=0.4
lr=1
binomalLearning<-function(nSamples,p,lr){
obs<-rbinom(nSamples,1,p)
prior<-matrix(0.5,1,2)#shape parameters of the beta distribution.
for(i in 1:length(obs)){
if (as.logical(obs[i])==T){
prior=rbind(prior,c(lr*1+prior[i,1],prior[i,2]))
}#if sub saw positive outcome add a lr weighted value to the
else if (as.logical(obs[i])==F){
prior=rbind(prior,c(prior[i,1],lr*1+prior[i,2]))
}#if sub saw positive outcome add a lr weighted value to the
}
cbind(prior,append(obs,99,after=0))#beta shape parameter and observation.
}
sub<-binomalLearning(nSamples,p,lr)
pl.beta(sub[nrow(sub),1],sub[nrow(sub),2])
## Visualization, including limit cases:
pl.beta <- function(a,b, asp = if(isLim) 1, ylim = if(isLim) c(0,1.1)) {
if(isLim <- a == 0 || b == 0 || a == Inf || b == Inf) {
eps <- 1e-10
x <- c(0, eps, (1:7)/16, 1/2+c(-eps,0,eps), (9:15)/16, 1-eps, 1)
} else {
x <- seq(0, 1, length = 1025)
}
fx <- cbind(dbeta(x, a,b), pbeta(x, a,b), qbeta(x, a,b))
f <- fx; f[fx == Inf] <- 1e100
matplot(x, f, ylab="", type="l", ylim=ylim, asp=asp,
main = sprintf("[dpq]beta(x, a=%g, b=%g)", a,b))
abline(0,1,     col="gray", lty=3)
abline(h = 0:1, col="gray", lty=3)
legend("top", paste0(c("d","p","q"), "beta(x, a,b)"),
col=1:3, lty=1:3, bty = "n")
invisible(cbind(x, fx))
}
marbleData
setwd("/Users/ciranka/Documents/R/SocialModels/Uncertainty/MR_behav/")
allfiles<-list.files()#get all files in the working directory.
relevantFiles<-list()#create an object where you store the relevant fil
j=1# set a counter which counts hits seperately.
for(i in 1:length(allfiles)){
relevant<-substr(allfiles[i],nchar(allfiles[i])-7,nchar(allfiles[i]))
if (relevant == ending){
relevantFiles[j]=allfiles[i]
j=j+1
}
}
marbleData<-data.frame()
#here there was a tiny problem. Raw data txt files had varying header lengths. i deleted them manually but for recap:
#odd ones out are:85,70,47,38,31,19,4
for(i in 1:length(relevantFiles)){
#because everyone has this habit of not saving the subjectspecifics in the data, retrieve them from the text file.
subi<-read.table(as.character(relevantFiles[i]),skip=1,header=T)#
subi$subject<-substr(as.character(relevantFiles[i]),4,7)#between character 4 and 7 the subjnumber is hidden
subi$run<-substr(as.character(relevantFiles[i]),9,9)#at char 9 here is the run hidden
marbleData<-rbind(marbleData,subi)#concatenate.
}
setwd("/Users/ciranka/Documents/R/SocialModels/Uncertainty/MR_behav/")
setwd("/Users/ciranka/Documents/R/SocialModels/Uncertainty/MR_behav/")
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
install.packages("tidyverse")
install.packages("dbplyr")
install.packages("tibble")
install.packages("tidyverse")
library(dplyr)
library(tidyr)
install.packages("dplyr")
library(dplyr)
library(tibble)
install.packages("tibble")
install.packages("utf8")
install.packages("nycflights13")
install.packages("nycflights13")
install.packages("tidyverse")
install.packages("utf8")
install.packages("RGtk2", depen=T, type="source")
install.packages("RGtk2", depen=T, type="source")
install.packages("RGtk2Extras")
install.packages("tidyverse")
install.packages("gcc")
install.packages("utf8")
load("/Users/ciranka/Documents/Experiments Java Script/OCU/socialInfo.RData")
dataExp
unique(dataExp)
unique(dataExp$valueGamble)
unique(dataExp[,c('valueGamble','probAv')])
UniqueConditions=unique(dataExp[,c('valueGamble','probAv')])
UniqueConditions$Conditions=1:length(UniqueConditions)
UniqueConditions
UniqueConditions$Conditions=1:length(UniqueConditions$valueGamble)
UniqueConditions
dataExp[dataExp$valueGamble==UniqueConditions$valueGamble && dataExp$probAv==UniqueConditions$probAv]$Condition=UniqueConditions[dataExp$valueGamble==UniqueConditions$valueGamble && dataExp$probAv==UniqueConditions$probAv]$Conditions
dataExp[dataExp$valueGamble==UniqueConditions$valueGamble && dataExp$probAv==UniqueConditions$probAv,]$Condition=UniqueConditions[dataExp$valueGamble==UniqueConditions$valueGamble && dataExp$probAv==UniqueConditions$probAv,]$Conditions
dataExp[dataExp$valueGamble==UniqueConditions$valueGamble && dataExp$probAv==UniqueConditions$probAv]
for (i in 1:length(dataExp)){
i
}
for (i in 1:length(dataExp)){
print(i)
}
for (i in 1:length(dataExp$valueGamble)){
print(i)
}
for (i in 1:length(dataExp$valueGamble)){
for(j in 1:length(UniqueConditions$valueGamble))
if((dataExp$valueGamble[i]==UniqueConditions[j]$valueGamble) && (dataExp[i]$probAv)==UniqueConditions[j]$probAv)
{
dataExp$Condition[i]=UniqueConditions$Conditions[j]
}
}
for (i in 1:length(dataExp$valueGamble)){
for(j in 1:length(UniqueConditions$valueGamble))
if((dataExp$valueGamble[i]==UniqueConditions[j]$valueGamble) && (dataExp[i]$probAv==UniqueConditions[j]$probAv))
{
dataExp$Condition[i]=UniqueConditions$Conditions[j]
}
}
for (i in 1:length(dataExp$valueGamble)){
for(j in 1:length(UniqueConditions$valueGamble))
if((dataExp$valueGamble[i]==UniqueConditions$valueGamble[j]) && (dataExp$probAv[i]==UniqueConditions$probAv[j]))
{
dataExp$Condition[i]=UniqueConditions$Conditions[j]
}
}
dataExp
unique(dataExp$probAv)
library(jsonlite)
JSONCondi <- toJSON(UniqueConditions)
JSONCondi
JSONCondi <- toJSON(dataExp)
setwd("/Users/ciranka/Documents/Experiments Java Script/OCU/")
save(JSONCondi, file="JSONCondi.json")
save(JSONCondi, file="JSONCondi.JSON")
write(JSONCondi, "JSONCondi.json")
dataExp[dataExp$ppn]
dataExp[dataExp$ppn,]
dataExp[dataExp$ppn=="101",]
dataExp[dataExp$ppn=="101",]$choice
mean(dataExp[dataExp$ppn=="101",]$choice)
unique(dataExp$ppn)
length(unique(dataExp$ppn))
length(unique(dataExp$ppn))*2
("/Users/ciranka/Documents/Experiments Java Script/OCU/ModelParamsFull.RData")
load("/Users/ciranka/Documents/Experiments Java Script/OCU/ModelParamsFull.RData")
dataExp
ModelParamsFull
ModelParamsFull[ModelParamsFull$Ind0_Soc1==0,]
subset(ModelParamsFull[ModelParamsFull$Ind0_Soc1==0,],c("valueGamble","ppn","choice","probAv")
)
?subset
subset(ModelParamsFull[ModelParamsFull$Ind0_Soc1==0,],select=c("valueGamble","ppn","choice","probAv"))
DataExp=subset(ModelParamsFull[ModelParamsFull$Ind0_Soc1==0,],select=c("valueGamble","ppn","choice","probAv"))
setwd("/Users/ciranka/Documents/Experiments Java Script/OCU/")
load("/Users/ciranka/Documents/Experiments Java Script/OCU/ModelParamsFull.RData")
# identify the unique conditions
UniqueConditions=unique(dataExp[,c('valueGamble','probAv')])
dataExp=subset(ModelParamsFull[ModelParamsFull$Ind0_Soc1==0,],select=c("valueGamble","ppn","choice","probAv"))# only choose the independent gambles
for (i in 1:length(DataExp$valueGamble)){
for(j in 1:length(UniqueConditions$valueGamble))
if((dataExp$valueGamble[i]==UniqueConditions$valueGamble[j]) && (dataExp$probAv[i]==UniqueConditions$probAv[j]))
{
dataExp$Condition[i]=UniqueConditions$Conditions[j]
}
}
library(jsonlite)
JSONCondi <- toJSON(UniqueConditions)
write(JSONCondi, "JSONCondi.json")
dataExp
for (i in 1:length(DataExp$valueGamble)){
for(j in 1:length(UniqueConditions$valueGamble))
if((dataExp$valueGamble[i]==UniqueConditions$valueGamble[j]) && (dataExp$probAv[i]==UniqueConditions$probAv[j]))
{
dataExp$Condition[i]=UniqueConditions$Conditions[j]
}
}
UniqueConditions
UniqueConditions$Conditions=1:length(UniqueConditions$valueGamble)
dataExp=subset(ModelParamsFull[ModelParamsFull$Ind0_Soc1==0,],select=c("valueGamble","ppn","choice","probAv"))# only choose the independent gambles
UniqueConditions=unique(dataExp[,c('valueGamble','probAv')])
UniqueConditions$Conditions=1:length(UniqueConditions$valueGamble) #add a conditiondummy
for (i in 1:length(DataExp$valueGamble)){
for(j in 1:length(UniqueConditions$valueGamble))
if((dataExp$valueGamble[i]==UniqueConditions$valueGamble[j]) && (dataExp$probAv[i]==UniqueConditions$probAv[j]))
{
dataExp$Condition[i]=UniqueConditions$Conditions[j]
}
}
library(jsonlite)
JSONCondi <- toJSON(dataExp)
write(JSONCondi, "JSONCondi.json")
mean(dataExp[dataExp$ppn=="101",]$choice)
mean(dataExp[dataExp$ppn=="102",]$choice)
mean(dataExp[dataExp$ppn=="103",]$choice)
mean(dataExp[dataExp$ppn=="104",]$choice)
mean(dataExp[dataExp$ppn=="105",]$choice)
mean(dataExp[dataExp$ppn=="107",]$choice)
mean(dataExp[dataExp$ppn=="106",]$choice)
